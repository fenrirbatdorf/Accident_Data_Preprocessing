{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw data\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\bkb3\\\\Documents\\\\US_Accidents_March23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to MD based data only\n",
    "df = data[data[\"State\"]=='MD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b14b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data, remove duplicate rows\n",
    "df.head()\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all unique values for all columns\n",
    "for col in df.columns:\n",
    "    print(df[col].value_counts(sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worth keeping weather time stamp? Examine value types\n",
    "print(df[\"Weather_Timestamp\"].value_counts(sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all values of all remaining cols\n",
    "for col in df.columns:\n",
    "    print(df[col].value_counts(sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51733dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date/time conversion, calculate length of traffic holdup in hours/fractions of an hour\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'],format='mixed')\n",
    "df['End_Time'] = pd.to_datetime(df['End_Time'], format = 'mixed')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for all cols again\n",
    "for col in df.columns:\n",
    "    print(df[col].value_counts(sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error checking of date time error I was getting over and over (One row had date and time recorded with higher\n",
    "# granularity)\n",
    "df.iloc[46456, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year, month, and day columns out of date time\n",
    "df['Year'], df['Month'], df['Day'] = df['Start_Time'].dt.year, df['Start_Time'].dt.month, df['Start_Time'].dt.day\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def season(x):\n",
    "    tomap = {1:\"Winter\",2:\"Winter\",3:\"Spring\",4:\"Spring\",5:\"Spring\",6:\"Summer\",7:\"Summer\",8:\"Summer\",9:\"Fall\",10:\"Fall\",11:\"Fall\",12:\"Winter\"}\n",
    "    if x in tomap:\n",
    "        return tomap[x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab801e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'],format='mixed')\n",
    "df['Month'] = df['Start_Time'].dt.month\n",
    "df[\"Season\"] = df.Month.apply(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert street names to strings\n",
    "df['Street'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5eccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def street_type(x):\n",
    "    if \"Tunl\" in x or \"Tunnel\" in x:\n",
    "        return \"Tunnel\"#nsew(x,\"Tunnel\")\n",
    "    if \"I-\" in x:\n",
    "        return \"Interstate\"\n",
    "    if \" Dr \" in x or \" Ave \" in x or \" Rd \" in x or \" Xing\" in x or \" Way \" in x:\n",
    "        return \"local\"\n",
    "    if \"Beltway\" in x:\n",
    "        return \"Beltway\"\n",
    "    if \"US-\" in x[:4] or \"MD-\" in x[:4] or \"Expy\" in x:\n",
    "        return \"Highway\"\n",
    "    if \"Hwy\" in x or \"Fwy\" in x or \"Highway\" in x or \"Route\" in x or \"Connector\" in x or \"Byp\" in x or \"Bypass\" in x or \"Gtwy\" in x or \"Brg\" in x or \"Trwy\" in x:\n",
    "        return \"Highway\"\n",
    "    if x[-2:] in [\"St\",\"Pl\",\"Dr\",\"Sq\"]:\n",
    "        return \"local\"\n",
    "    if \"Ln\" in x or \"Rd\" in x or \"Ct\" in x:\n",
    "        return \"local\"\n",
    "    if x[-3:] in [\"Way\",\"Cir\",\"Ter\",\"Trl\",\"Plz\",\"Aly\",\"Grn\",\"way\",\"Ext\"]:\n",
    "        return \"local\"\n",
    "    if \"Ave\" in x[-5:] or \" Blvd\" in x or \" Pike\" in x or \"Pkwy\" in x or \"Crse\" in x or \"Alameda\" in x:\n",
    "        return \"large_local\"\n",
    "    if \"Garth\" in x or \"Spell\" in x or \"Psge\" in x or \"Chase\" in x or \"Cutoff\" in x or \"Spire\" in x:\n",
    "        return \"local\"\n",
    "    if \"Overlook\" in x or \"Battlefield\" in x or \"Cemetery\" in x or \"Park\" in x or \"Base\" in x or \"Concourse\" in x or \"Memorial\" in x:\n",
    "        return \"special\"\n",
    "    if \"Loop\" in x or \"Greenway\" in x or \"West\" in x or \"ville\" in x or \"wood\" in x:\n",
    "        return \"local\"\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b379339",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Street\"] = df[\"Street\"].astype(str)\n",
    "df[\"Street\"] = df.Street.apply(street_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d473c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[~df.Street.str.contains(\"special\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Street.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper formatting of county names (I took an educated guess that \\\"Baltimore\\\" referred to Balt. County and not City, this\n",
    "# may not be correct)\n",
    "\n",
    "def replace_misspelled(text):\n",
    "    corrections = {\"St Mary's\": \"St. Marys\", \"Prince George's\": \"Prince Georges\", \"Queen Anne's\": \"Queen Annes\",\n",
    "                   \"Baltimore (City)\":\"Baltimore City\",\"Saint Mary's\":\"St Marys\", \"Baltimore County\":\"Baltimore\"}\n",
    "    for word, correction in corrections.items():\n",
    "        text = text.replace(word, correction)\n",
    "    return text\n",
    "\n",
    "df['County'] = df['County'].apply(replace_misspelled)\n",
    "df[\"County\"].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot function (Could not find the one line function Dr. Olsen referred to)\n",
    "def onehotinator(df, string):\n",
    "    one_hot = pd.get_dummies(df[string], prefix=string)\n",
    "    one_hot = one_hot.astype(int)\n",
    "    df = df.join(one_hot)\n",
    "    df = df.drop(columns=string)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae156ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean column feature for if snowy OR icy conditions were present\n",
    "lst = ['Snow', 'Freezing', 'Ice', 'Wintry', 'Hail', 'Sleet']\n",
    "df['Snow_Ice']=np.where(df['Weather_Condition'].str.contains('|'.join(lst), na=False), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf30529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean column feature for if mist OR fog OR haze was present, OR if visibility in miles was less than 0.001 mi\n",
    "lst = ['Fog', 'Mist', 'Haze', 'Heavy']\n",
    "df['Low_Vis_Weather']=np.where((df['Weather_Condition'].str.contains('|'.join(lst), na=False) | df[\"Visibility(mi)\"] < 0.001), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038caded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean column feature for if road conditions were wet (weather was rain/storm related, OR there was more than 0 inches\n",
    "# of precipitation at time of crash)\n",
    "lst = ['Rain', 'Drizzle', 'Thunderstorm', 'T-Storm', 'Precipitation']\n",
    "df['Rainy']=np.where((df['Weather_Condition'].str.contains('|'.join(lst), na=False) | df['Precipitation(in)'] >= 0.01), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba26db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns to delete\n",
    "df.head()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddadce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all but one day/night column\n",
    "# Col measures were: \n",
    "# Sun up/down based on the time of day\n",
    "# Sun up/down based on street lights being needed or not\n",
    "# Sun up/down based on being able to see the horizon at sea\n",
    "# Sun up/down based on being able to see stars\n",
    "df = df.drop(columns=['Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight', 'Weather_Condition'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for data conversion\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f151ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all boolean columns to 1/0 integer columns and only day/night column to 1/0\n",
    "\n",
    "bools = ['Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', \n",
    "         'Traffic_Signal', 'Turning_Loop', 'Low_Vis_Weather', 'Snow_Ice', 'Rainy']\n",
    "\n",
    "for i in bools:\n",
    "    df = df.astype({i:int})\n",
    "    df['Sunrise_Sunset'] = df.Sunrise_Sunset.map({'Day':1,'Night':0})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804dc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slowdown_level(row):\n",
    "    return sum([row['Bump'],row[\"Crossing\"], \\\n",
    "    row[\"Give_Way\"], row[\"Junction\"], row[\"Railway\"],  \\\n",
    "    row['Roundabout'], row[\"Stop\"], row[\"Traffic_Calming\"], \\\n",
    "    row[\"Traffic_Signal\"], row[\"Turning_Loop\"]])\n",
    "df['Qty_Slowing_Elements'] = df.apply(slowdown_level, axis=1)\n",
    "df['Qty_Slowing_Elements'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for what to do next\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6980328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for feature engineering ideas\n",
    "max(df['Temperature(F)']), min(df['Temperature(F)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24699eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for feature engineering ideas\n",
    "df['Wind_Chill(F)'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479577de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column for potentially freezing conditions if temp at accident time was below freezing OR windchill was below freezing\n",
    "# OR if the previous column checking for snow and ice was true snow/ice present\n",
    "\n",
    "def freezing(row):\n",
    "   if (row[\"Temperature(F)\"]<=32) | (row[\"Wind_Chill(F)\"]<=32) | (row[\"Snow_Ice\"] == 1):\n",
    "      return 1\n",
    "   else:\n",
    "      return 0\n",
    "df['Freezing'] = df.apply(freezing, axis=1)\n",
    "df[\"Freezing\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to a technical ctrl+z error, I lost all instances of removing columns as I went, this is a quick fix (I didn't remember\n",
    "# which columns were removed at which points)\n",
    "df.isnull().sum()\n",
    "df.columns\n",
    "for col in df.columns:\n",
    "    print(df[col].name, df[col].isnull().sum())\n",
    "df = df.drop(columns=['ID','Source','Start_Time','End_Time','Start_Lat','Start_Lng','End_Lat','End_Lng','Weather_Timestamp',\n",
    "                      'Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Direction',\n",
    "                      'Wind_Speed(mph)','Precipitation(in)','Description','Street','City','State','Zipcode','Country','Timezone',\n",
    "                      'Airport_Code'])\n",
    "\n",
    "# Dropping remaining null values left\n",
    "\n",
    "df = df.dropna(how='any')\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7271a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(df[col].name, df[col].isnull().sum())\n",
    "\n",
    "# Just to be safe, drop duplicates again\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.dtypes\n",
    "# Somehow sun up/down col was a float, quick fix to be int\n",
    "df[\"Sunrise_Sunset\"]=df[\"Sunrise_Sunset\"].astype(int)\n",
    "df.dtypes\n",
    "# Checked min and max of distance to decide on binning by hand or not\n",
    "print(df['Distance(mi)'].min(),df['Distance(mi)'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('fenrir_feat_eng.csv', index=False)\n",
    "#df.to_csv('Fenrir_Feat_Eng.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03157235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for thinking about CFD based division (We have 4 classes, and CFD only takes 2 at a time)\n",
    "df['Severity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30067a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f81e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng_class_merge(df, c1, c2, c3, c4):\n",
    "    df['Severity'] = df['Severity'].replace(c1, 0)\n",
    "    df['Severity'] = df['Severity'].replace([c2, c3, c4], 1)\n",
    "    df_c1 = df[df.Severity != 1]\n",
    "    df_c2 = df[df.Severity != 0]\n",
    "    df_c1.to_csv(f\"feat_eng_class{c1}.csv\", index = False)\n",
    "    df_c2.to_csv(f\"feat_eng_class{c2}_{c3}_{c4}_merge.csv\", index = False)\n",
    "\n",
    "    df['Severity'] = df['Severity'].replace(c2, 0)\n",
    "    df['Severity'] = df['Severity'].replace([c1, c3, c4], 1)\n",
    "    df_c1 = df[df.Severity != 1]\n",
    "    df_c2 = df[df.Severity != 0]\n",
    "    df_c1.to_csv(f\"feat_eng_class{c2}.csv\", index = False)\n",
    "    df_c2.to_csv(f\"feat_eng_class{c1}_{c3}_{c4}_merge.csv\", index = False)\n",
    "\n",
    "    df['Severity'] = df['Severity'].replace(c3, 0)\n",
    "    df['Severity'] = df['Severity'].replace([c2, c1, c4], 1)\n",
    "    df_c1 = df[df.Severity != 1]\n",
    "    df_c2 = df[df.Severity != 0]\n",
    "    df_c1.to_csv(f\"feat_eng_class{c3}.csv\", index = False)\n",
    "    df_c2.to_csv(f\"feat_eng_class{c2}_{c1}_{c4}_merge.csv\", index = False)\n",
    "\n",
    "    df['Severity'] = df['Severity'].replace(c4, 0)\n",
    "    df['Severity'] = df['Severity'].replace([c2, c3, c1], 1)\n",
    "    df_c1 = df[df.Severity != 1]\n",
    "    df_c2 = df[df.Severity != 0]\n",
    "    df_c1.to_csv(f\"feat_eng_class{c4}.csv\", index = False)\n",
    "    df_c2.to_csv(f\"feat_eng_class{c2}_{c3}_{c1}_merge.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feat_eng_class_merge(df, 1, 2, 3, 4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
